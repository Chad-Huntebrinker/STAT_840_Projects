---
title: "Chad Huntebrinker's Homework 9"
author: "Chad Huntebrinker"
date: "2024-11-11"
output:
  word_document: default
  html_document: default
---

Problem 8.4 and 8.5
```{r}
#Chad Huntebrinker

#Load the library and data in to R
library(readxl)
excel_data <- read_excel("Muscle_Mass_Data.xlsx")
excel_data$Age.c <- excel_data$Age-mean(excel_data$Age)

#Muscle_Mass ~ Age
#Fit the model
#Problem 8.4a
model_1 <- lm(Muscle_Mass~Age,data=excel_data)
sum_of_model_1 <- summary(model_1)
plot(Muscle_Mass~Age,data=excel_data)

model_2 <- lm(Muscle_Mass~Age.c + I(Age.c^2),data=excel_data)
sum_of_model_2 <- summary(model_2)
sum_of_model_2
#The quadratic function seems like a good fit.

#Problem 8.4b
summary(model_2)
qf(0.95, 2, 57)
#H0: B1 = B11 = 0, if F_score <= 3.158843
#Ha: not B1 and B11 = 0, if F_score > 3.158843
#F-statistic: 91.84 on 2 and 57 DF
91.84 > 3.158843
#Prove Ha is true.

#Problem 8.4c
predict(model_2, data.frame(Age.c = 48 - mean(excel_data$Age)),
        interval="confidence", se.fit = TRUE, level = 0.95)

#Problem 8.4d
predict(model_2, data.frame(Age.c = 48 - mean(excel_data$Age)),
        interval="prediction", se.fit = TRUE, level = 0.95)

#Problem 8.4e
sum_of_model_2
qt(0.975, 57)
#t* = 1.776
#H0: B11 = 0 if t* <= 2.002465
#Ha: B11 != 0, if t* > 2.002465
1.776 > qt(0.975, 57)
#We can conclude H0 is true

#Problem 8.4f
#Y = 207.350 − 2.96432X + .0148405X^2

#Problem 8.4g
cor(excel_data$Age, excel_data$Age^2)
cor(excel_data$Age.c, excel_data$Age.c^2)
#Yes, the use of a centered variable is helpful here.

#Problem 8.5a
plot(sum_of_model_2$residuals~predict(model_2))

qqnorm(residuals(model_2))
qqline(residuals(model_2))
#Both of these plots seem pretty standard and no need for concern.

#Problem 8.5b
#H0: E{Y} = B0 + B1x + B11x^2, F_score ≤ 1.875188
#HA: E{Y} != B0 + B1x + B11x^2, F_score > 1.875188
reduced_model <- lm(Muscle_Mass~Age.c, data = excel_data)
model_2_residuals <- residuals(model_2)
reduced_model_residuals <- residuals(reduced_model)

#Calculate MSLF
SST <- sum((excel_data$Muscle_Mass - mean(excel_data$Muscle_Mass))^2)

SSR_reduced <- sum((predict(reduced_model) - mean(excel_data$Muscle_Mass))^2)

SSLOF <- SST - SSR_reduced

dfLOF <- length(excel_data$Muscle_Mass) - length(coef(reduced_model)) - 1

MSLF <- SSLOF / dfLOF

#Calculate MSPE

MSPE <- mean((excel_data$Muscle_Mass - predict(model_2))^2)

F_score <- MSLF / MSPE

F_score > qf(0.95, 29, 28)
#Conclude H0

#Problem 8.5c
model_3 <- lm(Muscle_Mass~Age.c + I(Age.c^2) + I(Age.c^3), data = excel_data)
summary(model_3)
#Y = = 82.9273 − 1.26789x + .01504x2 + .000337x3
#t_score = 0.361
#H0: B111 = 0, t_score <= 2.003241
#Ha: B111 != 0, t_score > 2.003241
qt(0.975, 56)
0.361 <= qt(0.975, 56)
#Thus, we can conclude H0.
```

Problem 8.15 and 8.19
```{r}
#Chad Huntebrinker

#Load the library and data in to R
library(readxl)
excel_data <- read_excel("Copier_Maintenance_Data.xlsx")

#Problem 8.15a & b
model_1 <- lm(Total_Minutes~Number_Of_Copiers + Type_Of_Copier,data=excel_data)
summary(model_1)

#B0 is the intercept
#B1 is one of the slopes, specifically related with the number of copiers
#B2 is the other slopes, specifically related to the copier size.
#Y = -0.9225 + 15.0461X1 + 0.7587X2

#Problem 8.15c
mean(excel_data$Number_Of_Copiers)
predict(model_1, data.frame(Number_Of_Copiers = 5.111111, Type_Of_Copier = 0),
        interval="confidence", se.fit = TRUE, level = 0.95)
predict(model_1, data.frame(Number_Of_Copiers = 5.111111, Type_Of_Copier = 1),
        interval="confidence", se.fit = TRUE, level = 0.95)

#Problem 8.15d
#We would be interested in X1 because the service time is still impacted by the
#number of copiers even if we are interested in the effect of the type of copier (2 smaller
#copiers might have a similiar amount of service time as 1 larger one, for example).

#Problem 8.15e
temp_values <- excel_data$Number_Of_Copiers * excel_data$Type_Of_Copier
plot(Total_Minutes~temp_values, xlab = "Interaction Term", data = excel_data)
#Yes, there seems to be a pattern so a interaction term would be helpful.

#Problem 8.19a
model_2 <- lm(Total_Minutes~Number_Of_Copiers + Type_Of_Copier +
                Number_Of_Copiers*Type_Of_Copier, data = excel_data)
summary(model_2)
#Y = 2.8131 + 14.3394X1 - 8.1412X2 + 1.7774X1X2

#Problem 8.19b
#H0: B3 = 0, t_value <= qt(0.95, 41)
#HA: B3 != 0, t_value > qt(0.95, 41)
summary(model_2)
#t_value = 1.824
#p_value = 0.0755

qt(0.95, 41)

#Test H0
1.824 <= qt(0.95, 41)
#Reject H0, B3 != 0, keep it in the model.
```

Problem 8.37
```{r}
#Chad Huntebrinker

#Load the library and data in to R
library(readxl)
excel_data <- read_excel("CDI_Data.xlsx")

#Problem 8.37
excel_data$Pop_Density <- excel_data$Total_Pop / excel_data$Land_Area
Crime_Rate <- excel_data$Serious_Crimes / excel_data$Total_Pop
Pop_Density.c <- excel_data$Pop_Density - mean(excel_data$Pop_Density)
Unemployment.c <- excel_data$Unemployment - mean(excel_data$Unemployment)

model_1 <- lm(Crime_Rate~Pop_Density.c + Unemployment.c + I(Pop_Density.c^2) +
                I(Unemployment.c^2) + Pop_Density.c*Unemployment.c)
sum_of_model_1 <- summary(model_1)

plot(model_1$residuals~model_1$fitted.values)

sum_of_model_1
#Multiple R-squared:  0.2485,	Adjusted R-squared:  0.2398
#The second-order model appears to fit the data pretty well, although there are some outliers
#in the regression function

#Problem 8.37b
#H0: p_value > 0.01, drop the quadratic and interaction terms
#Ha: p_value < 0.01, keep the quadratic and interaction terms
reduced_model <- lm(Crime_Rate~Pop_Density.c + Unemployment.c)
anova(reduced_model, model_1)
#p_value = 0.02278 > 0.01
0.02278 > 0.01
#H0 is true, so drop those terms

#Problem 8.37c
#Problem doesn't specify whether these variables need to be centered or not.
#So, I will do both.
model_2 <- lm(Crime_Rate~Total_Pop + Land_Area + Unemployment + I(Total_Pop^2),
              data = excel_data)
summary(model_2)
#Multiple R-squared:  0.1444,	Adjusted R-squared:  0.1365

Total_Pop.c <- excel_data$Total_Pop - mean(excel_data$Total_Pop)
Land_Area.c <- excel_data$Land_Area - mean(excel_data$Land_Area)

model_3 <- lm(Crime_Rate~Total_Pop.c + Land_Area.c + Unemployment.c + I(Total_Pop.c^2))
summary(model_3)
#Multiple R-squared:  0.1444,	Adjusted R-squared:  0.1365

#While the coefficients are different, they both are on the lower end of the spectrum.
```


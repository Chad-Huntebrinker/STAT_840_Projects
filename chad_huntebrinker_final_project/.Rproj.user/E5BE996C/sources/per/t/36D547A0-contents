---
title: "Predicting Market Share of Packaged Food Product"
author: "Chad Huntebrinker"
date: "2024-12-10"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(readxl)
library(leaps)
library(psych)
library(lmtest)

excel_data <- read_excel("market_share.xlsx")
```

# Title
Predicting Market Share of a Packaged Food Product using assorted variables from 36 consecutive months.

# Abstract
Any profitable company is concerned with market share, especially with market shares of their products. For this report, an examination into the factors influencing the market share of a packaged food product using a multiple linear regression model is done. The data is collected from a national database (Nielsen), spans 36 months, and includes variables such as product price and amount of advertising exposure. The analysis focuses on determining how these factors impact the product's market share. By exploring the relationships between these variables, the report aims to provide valuable insights for company executives for their marketing strategies and improve market share. The conclusions drawn highlight the most significant influences on market performance, guiding future decision-making processes.

# Introduction
To consider what impacts market shares of a product, wide range of variables need to be considered. First off, market share itself is a measure of how much of a specific industry a product controls and is a percentage. For example, if a product sells 1,000 units of a product out of a total of 10,000 units in the market sector, then the market share for the product is 10%. The next variables that will be used to predict the market share of a product. Most of these variables are self explanatory. These would be the the month and year when this data was taken, the average monthly price (in dollars) of a product, whether there was a discount or some sort of promotion during the period. One variable, however, is the Gross Nielson Rating Points. The "[g]ross rating points (GRPs) quantify impressions as a percentage of a target audience, multiplied by the frequency of the audience seeing the ad. GRPs help marketers to understand the size of an audience that an ad impacts."[1]. Or in other words, it's a measurement used for the amount of advertising exposure a product receives. With these variables, this report aims to investigate whether market share of a product can be predicted.

## Primary Analysis Objectives
The Primary Analysis Objective is to investigate whether there is a linear association between market share of a product and variables related to a product and how it is advertised to see if accurate predictions can be made when given a set of values for these variables.

# Methods
## Data Sources
Data was collected from a national database (Nielsen) for 36 consecutive months. The product is from a large packaged foods manufacturer, although it is unknwon what the food is.

## Statistical Analysis
The data is avilable in .xlsx (Excel) format. The data analysis is completed using the software R (version 4.4.1) and this project focuses on multiple linear regression. Two variables were not included when creating the model: id number and month. Id number was not included due to it only being used to keep track of the row's information. Month was not included as it seemed to have little no impact on the market share. No missing values were found in the dataset. Automatic model selection methods were used to arrive to the final model and the model assumptions are compared and confirmed while reviewing and using the final model.

## Model Assumtions
All interfences are done using a significance value of 0.05.

## Primary Objective Analysis
First, this report will investigate the individual predictors and the response variable to verify the skewness and identify any outliers. It will then complete a check for multicollinearity before utilizing automatic model selection to find the most effective predictor variables to include in the model. After that, the report will complete other checks for this final model (like homorskedasticity, normality of the errors, and independence of error terms) before seeing how accurate the model predicts.

### Individual Predictor and Response Variable Investigation
```{r echo=FALSE}
par(mfrow = c(2, 2))
boxplot(excel_data$price, xlab = "Price Boxplot", horizontal = TRUE)
boxplot(excel_data$gnrpoints, xlab = "Gross Nielson rating points Boxplot", horizontal = TRUE)
boxplot(excel_data$marketshare, xlab = "Market share Boxplot", horizontal = TRUE)
hist(excel_data$marketshare, main = "Market share Histogram", xlab = "Market share (in percentage)")
par(mfrow = c(1, 1))
```
After reviewing the boxplots for the continuous predictor variables (Price and Gross Nielson), the overall spread is good. They both seem to have an outlier on the far right side of the graph, something to be mindful of and check if any outliers are impacting the model if they are included in the final model. Next, a quick check on the spread of our response variable (market share) has shown no outliers.


### Multicollinearity
```{r echo=FALSE}
pairs.panels(excel_data[,-1], ellipses = FALSE, density = FALSE)
temp <- c(1, 7, 8)
cor(excel_data[,-temp])
```
The next step is to look at both the relationship between market share and all the predictor variables along with the multicolineratiy between the predictor variables. First, there seems to be no or very little multicolineratiy between the predictor variables.  When it comes to the relationship between market share and the continuous predictor variables like gnrpoints and price, there doesn't seem to be much of a linear relationship between them individually. There seems to be some between market share and discount.

### Automatic Model Selection
```{r echo=FALSE}
#Check to see what are the best three subsets
auto_model <- regsubsets(marketshare~price + gnrpoints + discount + promotion + year,
                 data=excel_data)
sum_of_auto_model <- summary(auto_model)

#Check Cp, R^2, and BIC
sum_of_auto_model$cp
sum_of_auto_model$rsq
sum_of_auto_model$bic

#Model 3 is our best model
sum_of_auto_model$which[3,]
#Thus, we will include price, discount and promotion

#Create a model with those three variables in it
final_model <- lm(marketshare~price + discount + promotion, data = excel_data)
```
The next step is to find a model via automatic variable selection. This report utilized the "regsubsets" function from the "leap" package in R. This report also utilizes Mallow's Cp, adjusted R^2, and BIC. Both for Cp and BIC, lower values mean a better model, while a higher adjusted R^2 means a better model. When looking through the scores, this investigation found that the third model had the best results. After looking at what the third model includes, it was found that including price, discount, promotion, and an X-intercept would lead to the best model.

### Final Checks on the Model
```{r echo=FALSE}
#Check for outliers
#Check studentized residuals against the predicted values in the model
plot(fitted(final_model), rstudent(final_model),
     col = "lightgreen", pch = 16,
     xlab = expression(hat(Y)),
     ylab = "Studentized Residuals",
     ylim = c(-5, 5), main = "(a)",
     cex.lab = 0.7, cex = 0.5)
abline(h = 0, lty = 2, lwd = 3,
       col = "darkgray")
abline(h = 3, lty = 3, lwd = 3,
       col = "darkgray")
abline(h = -3, lty = 3, lwd = 3,
       col = "darkgray")

#Cook's Distance
print("Value to use for Cook's Distance:")
pf(0.95, df1 = 3, df2 = 33)
cat("\n")
cat("\n")

cooksd <- cooks.distance(final_model)
plot(cooksd, pch = "*", cex.main = 0.7, main = "(b)",
     ylab = "Cooks distance", cex.lab = 0.7)
text(x = 1:length(cooksd) + 1, y = cooksd, labels = ifelse(cooksd > 0.5722362, names(cooksd), ""),col = "yellow")


#Homorskedasticity
plot(fitted(final_model), residuals(final_model), main = "(c)",
     xlab = expression(hat(Y)), ylab = "Residuals")
abline(h = 0, col = "gray", lwd = 2,lty = 2)

#Normality
qqnorm(residuals(final_model), main = "(d)")
qqline(residuals(final_model), col = "blue", lwd = 2)
hist(residuals(final_model), main = "(e)", xlab = "Residuals")

#Independence of error terms
plot(residuals(final_model), type = "l", col = "blue", main = "(f)", ylab = "Residuals")

#Validate the model
predict(final_model, data.frame(price = 2.179, discount = 1, promotion = 0),
        level = 0.95, interval = "prediction")
print("Actual value: 2.69")
cat("\n")

predict(final_model, data.frame(price = 2.260, discount = 0, promotion = 1),
        level = 0.95, interval = "prediction")
print("Actual value: 2.42")
cat("\n")

predict(final_model, data.frame(price = 2.248, discount = 0, promotion = 0),
        level = 0.95, interval = "prediction")
print("Actual value: 2.45")
cat("\n")

predict(final_model, data.frame(price = 2.557, discount = 1, promotion = 0),
        level = 0.95, interval = "prediction")
print("Actual value: 2.72")
cat("\n")

predict(final_model, data.frame(price = 2.336, discount = 1, promotion = 1),
        level = 0.95, interval = "prediction")
print("Actual value: 3.05")
cat("\n")

#Check p-score
p_score <- coeftest(final_model)

abs(p_score[2,4]) <= 0.05
abs(p_score[3,4]) <= 0.05
abs(p_score[4,4]) <= 0.05
```
The first thing the report checked with this new model is the residuals. A graph is created for the studentized residuals (graph a) and Cook's Distance (graph b) to see if any data points have a huge influence on the model. After looking for any points within a range of ±3 for graph a or for any points with a yellow marks indicating they're outliers, but there are no such points.

The next checks this report looks at is the equal variance (graph c), the normaility of the errors (graph d and e), and the independence of the residuals (graph f). All of these graphs indicate that everything is okay.

The last checks required are the goodness of fit of the model and the accuracy. First, the p-test is done on the three different coefficients. A comparision between their p-value and 0.05 is done and it's found that each of their values are less than or equal to 0.05. Thus confirming the goodness of fit with the model. The final check this investigation does is with predictions. The model tries to predict what the market share of the product by giving data that's in the data. What's found is most of the predictions are close to the actual values (off by 0.1 or less) or within the upper and lower bounds. As a result, it seems to indicate the accuracy of the final model.

# General Discussion and Conclusions
So the final estimated regression function from this investigation would be:
```{r echo=FALSE}
cat("Ŷ = ", round(coefficients(final_model)[1], 2), 
      "+", round(coefficients(final_model)[2], 2), "X1", 
      "+", round(coefficients(final_model)[3], 2), "X2",
      "+", round(coefficients(final_model)[4], 2), "X3")
```
Where Ŷ is the market share, X1 represents the price, X2 indicates whether there was a discount, and X3 indicates whether there was a promotion. The report found that a multiple linear regression model faithfully explains the relationship between the predictor variables and the market share. The MSE (which is found in the Anova table below) is 0.02244, indicating a relatively good fit. The report also looked at different tests and checks like residual plots, Cook's Distance, and homorskedasticity (to name a few) to verify the model is both accurate and reliable. The model also predicted some of the values in the data and was found to be accurate as well. As a result, this report shows that the price of the product along with offered discounts and promotions impact the market share of a food product. This company could utlize this report as a way to make logical and data-driven decisions that will positively impact the market share of a product.

```{r echo=FALSE}
anova(final_model)
```

# Appendix: R-code
```{r eval=FALSE}
#Chad Huntebrinker
#Use signifigance value of 0.05

library(readxl)
library(leaps)
library(psych)
library(lmtest)

excel_data <- read_excel("market_share.xlsx")


#Check for outliers for continuous predictor variables (price, and gnrpoints)
par(mfrow = c(1, 2))
boxplot(excel_data$price, xlab = "Price Boxplot", horizontal = TRUE)
boxplot(excel_data$gnrpoints, xlab = "Gross Nielson rating points Boxplot", horizontal = TRUE)
par(mfrow = c(1, 1))
#Boxplots either show only potentially one outlier.

#Check boxplot for the response variable (marketshare)
par(mfrow = c(1, 2))
boxplot(excel_data$marketshare, xlab = "Market share Boxplot", horizontal = TRUE)
hist(excel_data$marketshare, main = "Market share Histogram", xlab = "Market share (in percentage)")
par(mfrow = c(1, 1))
#No outliers for the response variable

#Multicollinearity
pairs.panels(excel_data[,-1], ellipses = FALSE, density = FALSE)
temp <- c(1, 7, 8)
cor(excel_data[,-temp])
#there is some concern with multicollinearity with discount and market share

#Check to see what are the best three subsets
mr <- regsubsets(marketshare~price + gnrpoints + discount + promotion + year,
                 data=excel_data)
sum_of_mr <- summary(mr)

#Check Cp, R^2, and BIC
sum_of_mr$cp
sum_of_mr$rsq
sum_of_mr$bic

#Model 3 is our best model
sum_of_mr$which[3,]
#Thus, we will include price, discount and promotion

#Create a model with those three variables in it
final_model <- lm(marketshare~price + discount + promotion, data = excel_data)

#Check for outliers
#Check studentized residuals against the predicted values in the model
plot(fitted(final_model), rstudent(final_model),
     col = "lightgreen", pch = 16,
     xlab = expression(hat(Y)),
     ylab = "Studentized Residuals",
     ylim = c(-5, 5), main = "(a)",
     cex.lab = 0.7, cex = 0.5)
abline(h = 0, lty = 2, lwd = 3,
       col = "darkgray")
abline(h = 3, lty = 3, lwd = 3,
       col = "darkgray")
abline(h = -3, lty = 3, lwd = 3,
       col = "darkgray")

#Cook's Distance
print("Value to use for Cook's Distance:")
pf(0.95, df1 = 3, df2 = 33)
cat("\n")
cat("\n")

cooksd <- cooks.distance(final_model)
plot(cooksd, pch = "*", cex.main = 0.7, main = "(b)",
     ylab = "Cooks distance", cex.lab = 0.7)
text(x = 1:length(cooksd) + 1, y = cooksd, labels = ifelse(cooksd > 0.5722362, names(cooksd), ""),col = "yellow")


#Homorskedasticity
plot(fitted(final_model), residuals(final_model), main = "(c)",
     xlab = expression(hat(Y)), ylab = "Residuals")
abline(h = 0, col = "gray", lwd = 2,lty = 2)

#Normality
qqnorm(residuals(final_model), main = "(d)")
qqline(residuals(final_model), col = "blue", lwd = 2)
hist(residuals(final_model), main = "(e)", xlab = "Residuals")

#Independence of error terms
plot(residuals(final_model), type = "l", col = "blue", main = "(f)", ylab = "Residuals")

#Validate the model
predict(final_model, data.frame(price = 2.179, discount = 1, promotion = 0),
        level = 0.95, interval = "prediction")
print("Actual value: 2.69")
cat("\n")

predict(final_model, data.frame(price = 2.260, discount = 0, promotion = 1),
        level = 0.95, interval = "prediction")
print("Actual value: 2.42")
cat("\n")

predict(final_model, data.frame(price = 2.248, discount = 0, promotion = 0),
        level = 0.95, interval = "prediction")
print("Actual value: 2.45")
cat("\n")

predict(final_model, data.frame(price = 2.557, discount = 1, promotion = 0),
        level = 0.95, interval = "prediction")
print("Actual value: 2.72")
cat("\n")

predict(final_model, data.frame(price = 2.336, discount = 1, promotion = 1),
        level = 0.95, interval = "prediction")
print("Actual value: 3.05")
cat("\n")

#Check p-score
p_score <- coeftest(final_model)

abs(p_score[2,4]) <= 0.05
abs(p_score[3,4]) <= 0.05
abs(p_score[4,4]) <= 0.05
```

# Reference
[1] What is a gross rating point (GRP)? Available online.
https://www.adjust.com/glossary/gross-rating-point-grp/